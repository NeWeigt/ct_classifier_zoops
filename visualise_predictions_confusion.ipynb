{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd2b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac0532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"/home/Nele/code/ct_classifier_zoops/val_predictions_epoch_.json\"\n",
    "image_root = \"/mnt/class_data/Nele/ct_train_data\"\n",
    "category_dict = json.load(open(\"/home/Nele/code/scripts/DataPrep_Classifier/category_dict.json\", 'r'))\n",
    "inverse_category_dict = {v: k for k, v in category_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"/home/Nele/code/ct_classifier_zoops/runs/Jan23_03-24-49_cv4e-2026-student4/val_predictions\")\n",
    "files = sorted(results_dir.glob(\"val_predictions_epoch_*.json\"))\n",
    "files = files[::-1]  # reverse order to start with last epoch\n",
    "print(f\"Found {len(files)} files to process.\")\n",
    "\n",
    "for f in files[::1000]:\n",
    "    fp = f\"{results_dir}/{f.name}\"\n",
    "    ep_num_str = int(f.name.split('_')[-1].split('.')[0])\n",
    "    print(f\"Processing file: {fp} {f}\")\n",
    "    with open(fp, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        predictions = data['predictions']\n",
    "        ground_truths = data['labels']\n",
    "        confidences = data['confidences']\n",
    "        image_names = data['image_names']\n",
    "    # convert lists to numpy arrays for easier manipulation\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    # plot confusion matrix normalised over true labels\n",
    "    cm = confusion_matrix(ground_truths, predictions, normalize='true', labels=list(inverse_category_dict.keys()))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix ep:{ep_num_str}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(set(ground_truths)))\n",
    "    plt.xticks(tick_marks, [inverse_category_dict[idx] for idx in tick_marks])\n",
    "    # rotate x tick labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, [inverse_category_dict[idx] for idx in tick_marks])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig('confusion_matrix.png')\n",
    "    # plot histogram of confidences for correct and incorrect predictions\n",
    "    correct_confidences = [confidences[i] for i in range(len(confidences)) if predictions[i] == ground_truths[i]]\n",
    "    incorrect_confidences = [confidences[i] for i in range(len(confidences)) if predictions[i] != ground_truths[i]]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(correct_confidences, bins=20, alpha=0.7, label='Correct Predictions', color='g')\n",
    "    plt.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect Predictions', color='r')\n",
    "    plt.title(f'Prediction Confidences ep:{ep_num_str}')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Number of Predictions')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plot for each class the distribution of confidences for correct and incorrect predictions\n",
    "    for class_idx in inverse_category_dict.keys():\n",
    "        class_name = inverse_category_dict[class_idx]\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        correct_indices = [i for i in range(len(confidences)) if predictions[i] == ground_truths[i] == class_idx]\n",
    "        incorrect_indices = [i for i in range(len(confidences)) if predictions[i] != class_idx and ground_truths[i] == class_idx]\n",
    "        class_correct_confidences = [confidences[i] for i in correct_indices]\n",
    "        class_incorrect_confidences = [confidences[i] for i in incorrect_indices]\n",
    "        # class_correct_confidences = [confidences[i] for i in range(len(confidences)) if predictions[i] == ground_truths[i] == class_idx]\n",
    "        # class_incorrect_confidences = [confidences[i] for i in range(len(confidences)) if ground_truths[i] == class_idx and predictions[i] != class_idx]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(class_correct_confidences, bins=20, alpha=0.7, label='Correct Predictions', color='g')\n",
    "        plt.hist(class_incorrect_confidences, bins=20, alpha=0.7, label='Incorrect Predictions', color='r')\n",
    "        plt.title(f'Prediction Confidences for class \"{class_name}\" ep:{ep_num_str}')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Number of Predictions')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # show the 5 most confident correct and incorrect predictions for this class\n",
    "        correct_indices_sorted = sorted(correct_indices, key=lambda i: confidences[i], reverse=True)[:5]\n",
    "        incorrect_indices_sorted = sorted(incorrect_indices, key=lambda i: confidences[i], reverse=True)[:5]\n",
    "        print(f\"Class '{class_name}' - Top 5 Correct Predictions:\")\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 3))\n",
    "        for i in correct_indices_sorted:\n",
    "            print(f\"  Image: {image_names[i]}, Confidence: {confidences[i]:.4f}\")\n",
    "            img = plt.imread(f\"{image_root}/{image_names[i]}\")\n",
    "            ax[correct_indices_sorted.index(i)].imshow(img, cmap='gray')\n",
    "            ax[correct_indices_sorted.index(i)].axis('off')\n",
    "            ax[correct_indices_sorted.index(i)].set_title(f\"Conf: {confidences[i]:.4f}\")\n",
    "        plt.suptitle(f\"Class '{class_name}' - Top 5 Correct Predictions\")\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 3))\n",
    "        print(f\"Class '{class_name}' - Top 5 Incorrect Predictions:\")\n",
    "        for i in incorrect_indices_sorted:\n",
    "            print(f\"  Image: {image_names[i]}, Confidence: {confidences[i]:.4f} (GT: {inverse_category_dict[ground_truths[i]]}) {inverse_category_dict[predictions[i]]}\")\n",
    "            img = plt.imread(f\"{image_root}/{image_names[i]}\")\n",
    "            ax[incorrect_indices_sorted.index(i)].imshow(img, cmap='gray')\n",
    "            ax[incorrect_indices_sorted.index(i)].axis('off')\n",
    "            ax[incorrect_indices_sorted.index(i)].set_title(f\"Conf: {confidences[i]:.4f} \\ngt:{inverse_category_dict[ground_truths[i]]} pr:{inverse_category_dict[predictions[i]]}\")\n",
    "        plt.suptitle(f\"Class '{class_name}' - Top 5 Incorrect Predictions\\n\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a40c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce376906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confidence score of the model per class\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mean confidence for each true class\n",
    "conf_arr = np.array(confidences)\n",
    "gt = np.array(ground_truths)\n",
    "\n",
    "class_ids = sorted(inverse_category_dict.keys())\n",
    "class_names = [inverse_category_dict[c] for c in class_ids]\n",
    "mean_conf_true = [conf_arr[gt == c].mean() if (gt == c).any() else np.nan for c in class_ids]\n",
    "std_conf_true = [conf_arr[gt == c].std() if (gt == c).any() else np.nan for c in class_ids]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(class_names, mean_conf_true, color=\"steelblue\")\n",
    "plt.errorbar(class_names, mean_conf_true, yerr=std_conf_true, fmt=\"o\", color=\"red\", alpha=0.5)\n",
    "plt.title(f\"Mean confidence by class ep:{ep_num_str}\")\n",
    "plt.ylabel(\"Mean confidence\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# It is the mean confidence score of each sample in this human label class, regardles of class it got predicted as. \n",
    "# It does not show what the scores are for the predicted classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confidence score of the model per class\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mean confidence for each true class\n",
    "conf_arr = np.array(confidences)\n",
    "pred = np.array(predictions)\n",
    "\n",
    "class_ids = sorted(inverse_category_dict.keys())\n",
    "class_names = [inverse_category_dict[c] for c in class_ids]\n",
    "mean_conf_true = [conf_arr[pred == c].mean() if (pred == c).any() else np.nan for c in class_ids]\n",
    "std_conf_true = [conf_arr[pred == c].std() if (pred == c).any() else np.nan for c in class_ids]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(class_names, mean_conf_true, color=\"steelblue\")\n",
    "plt.errorbar(class_names, mean_conf_true, yerr=std_conf_true, fmt=\"o\", color=\"red\", alpha=0.5)\n",
    "plt.title(f\"Mean confidence by class ep:{ep_num_str}\")\n",
    "plt.ylabel(\"Mean confidence\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(gt, pred)   # shape [C, C]\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "class_avg_acc = np.nanmean(per_class_acc)\n",
    "\n",
    "# Table\n",
    "df = pd.DataFrame({\n",
    "    \"Class\": class_names,                 # list of class labels\n",
    "    \"Accuracy\": per_class_acc\n",
    "})\n",
    "\n",
    "print(df)\n",
    "print(\"\\nClass-average accuracy:\", class_avg_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ecology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
